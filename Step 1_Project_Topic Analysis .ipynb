{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# China"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/becca/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3254: DtypeWarning: Columns (15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    }
   ],
   "source": [
    "#Read Files\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def file_reader(path):\n",
    "    df=pd.DataFrame()\n",
    "    chunksize = 10 ** 6\n",
    "    for chunk in pd.read_csv(path, chunksize=chunksize):\n",
    "        df = df.append(chunk)\n",
    "    return df\n",
    "df=file_reader('/Volumes/BECCA/Project/Data/Input/01_Raw data/China_052020.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop columns, expect 'text' & 'tweet_language' & 'tweet_time'\n",
    "for i in df:\n",
    "    if i != 'tweet_text' :\n",
    "        if i != 'tweet_language':\n",
    "            if i != 'tweet_time':\n",
    "                df=df.drop([i], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_language</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zh</td>\n",
       "      <td>郭瘟鬼这个大骗子，打着换汇的幌子通过他的地下钱庄骗钱，张口就来，每天就是在炫耀自己的骗术，殊...</td>\n",
       "      <td>2019-12-12 01:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>und</td>\n",
       "      <td>https://t.co/hNm6nUAiE3</td>\n",
       "      <td>2020-01-06 06:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>und</td>\n",
       "      <td>https://t.co/UKuYfubjgh</td>\n",
       "      <td>2020-01-07 10:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>und</td>\n",
       "      <td>https://t.co/KtpavahflW</td>\n",
       "      <td>2020-01-06 01:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ja</td>\n",
       "      <td>秋 https://t.co/36ISekYyC9</td>\n",
       "      <td>2019-11-05 00:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348603</th>\n",
       "      <td>zh</td>\n",
       "      <td>每一个普通的改变，都将改变普通。因为从决心变得更好那一刻开始，我们就已经与一个全新的自己不期...</td>\n",
       "      <td>2019-11-12 07:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348604</th>\n",
       "      <td>zh</td>\n",
       "      <td>受黑暴示威影響，本港訪港旅客數字連跌4個月，香港旅遊及發展局公布10月訪港旅客錄得逾331萬...</td>\n",
       "      <td>2019-11-30 01:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348605</th>\n",
       "      <td>zh</td>\n",
       "      <td>女孩一定要和满眼都是你的人在一起啊</td>\n",
       "      <td>2019-10-01 01:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348606</th>\n",
       "      <td>zh</td>\n",
       "      <td>你这么可爱，我一定要爱你，我不允許你自己擅自做主</td>\n",
       "      <td>2019-07-25 01:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348607</th>\n",
       "      <td>zh</td>\n",
       "      <td>我们学校厕所少，每次上厕所都要排队。。。有次闺密尿急，好不容易排到前面就一个人了，竟然有人往...</td>\n",
       "      <td>2020-02-20 01:41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>348608 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_language                                         tweet_text  \\\n",
       "0                  zh  郭瘟鬼这个大骗子，打着换汇的幌子通过他的地下钱庄骗钱，张口就来，每天就是在炫耀自己的骗术，殊...   \n",
       "1                 und                            https://t.co/hNm6nUAiE3   \n",
       "2                 und                            https://t.co/UKuYfubjgh   \n",
       "3                 und                            https://t.co/KtpavahflW   \n",
       "4                  ja                          秋 https://t.co/36ISekYyC9   \n",
       "...               ...                                                ...   \n",
       "348603             zh  每一个普通的改变，都将改变普通。因为从决心变得更好那一刻开始，我们就已经与一个全新的自己不期...   \n",
       "348604             zh  受黑暴示威影響，本港訪港旅客數字連跌4個月，香港旅遊及發展局公布10月訪港旅客錄得逾331萬...   \n",
       "348605             zh                                  女孩一定要和满眼都是你的人在一起啊   \n",
       "348606             zh                           你这么可爱，我一定要爱你，我不允許你自己擅自做主   \n",
       "348607             zh  我们学校厕所少，每次上厕所都要排队。。。有次闺密尿急，好不容易排到前面就一个人了，竟然有人往...   \n",
       "\n",
       "              tweet_time  \n",
       "0       2019-12-12 01:56  \n",
       "1       2020-01-06 06:05  \n",
       "2       2020-01-07 10:52  \n",
       "3       2020-01-06 01:24  \n",
       "4       2019-11-05 00:59  \n",
       "...                  ...  \n",
       "348603  2019-11-12 07:08  \n",
       "348604  2019-11-30 01:24  \n",
       "348605  2019-10-01 01:40  \n",
       "348606  2019-07-25 01:37  \n",
       "348607  2020-02-20 01:41  \n",
       "\n",
       "[348608 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just use the tweet in English\n",
    "tweets=[]\n",
    "for i in range(len(df)):\n",
    "    if df.tweet_language[i] == 'en':\n",
    "        l=[]\n",
    "        l.append(df.tweet_text[i])\n",
    "        l.append(df.tweet_time[i])\n",
    "        tweets.append(l)\n",
    "tweets=pd.DataFrame(tweets, columns=['tweet_text','tweet_time'])\n",
    "tweets['suspend']=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_time</th>\n",
       "      <th>suspend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Necessity is the mother of invention</td>\n",
       "      <td>2019-10-18 21:21</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#病毒  Constantly defying the will of the citize...</td>\n",
       "      <td>2020-03-26 05:09</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#肺炎  #病毒  The priceless life should not be the...</td>\n",
       "      <td>2020-03-28 02:37</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Want to see the walking dead, but feel a littl...</td>\n",
       "      <td>2019-10-30 05:57</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Maybe should call that Kekong? #HongKong #Keki...</td>\n",
       "      <td>2020-01-03 01:39</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32921</th>\n",
       "      <td>Look before you leap. First think, then act.</td>\n",
       "      <td>2019-05-22 08:30</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32922</th>\n",
       "      <td>I was for all intensive purposes addicted to t...</td>\n",
       "      <td>2019-04-21 03:23</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32923</th>\n",
       "      <td>.Live a noble and honest life.Reviving past ti...</td>\n",
       "      <td>2019-06-17 08:48</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32924</th>\n",
       "      <td>her, and she drooped and would have sunk down ...</td>\n",
       "      <td>2019-10-24 01:19</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32925</th>\n",
       "      <td>Whаtever you do, do it well. – Wаlt Disney</td>\n",
       "      <td>2019-10-11 10:16</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32926 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweet_text        tweet_time  \\\n",
       "0                   Necessity is the mother of invention  2019-10-18 21:21   \n",
       "1      #病毒  Constantly defying the will of the citize...  2020-03-26 05:09   \n",
       "2      #肺炎  #病毒  The priceless life should not be the...  2020-03-28 02:37   \n",
       "3      Want to see the walking dead, but feel a littl...  2019-10-30 05:57   \n",
       "4      Maybe should call that Kekong? #HongKong #Keki...  2020-01-03 01:39   \n",
       "...                                                  ...               ...   \n",
       "32921       Look before you leap. First think, then act.  2019-05-22 08:30   \n",
       "32922  I was for all intensive purposes addicted to t...  2019-04-21 03:23   \n",
       "32923  .Live a noble and honest life.Reviving past ti...  2019-06-17 08:48   \n",
       "32924  her, and she drooped and would have sunk down ...  2019-10-24 01:19   \n",
       "32925         Whаtever you do, do it well. – Wаlt Disney  2019-10-11 10:16   \n",
       "\n",
       "       suspend  \n",
       "0         True  \n",
       "1         True  \n",
       "2         True  \n",
       "3         True  \n",
       "4         True  \n",
       "...        ...  \n",
       "32921     True  \n",
       "32922     True  \n",
       "32923     True  \n",
       "32924     True  \n",
       "32925     True  \n",
       "\n",
       "[32926 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Analysis\n",
    "-to split file into different topic cluster, then get dataset under each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweet_text preprocessing\n",
    "import preprocessor as p\n",
    "\n",
    "for i,v in enumerate(tweets['tweet_text']):\n",
    "    tweets.loc[i,'text'] = p.clean(v)\n",
    "    \n",
    "import spacy\n",
    "# spacy.load('en_core_web_sm')\n",
    "from spacy.lang.en import English\n",
    "parser = English()\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens\n",
    "\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "import random\n",
    "text_data = []\n",
    "for i in range(len(tweets)):\n",
    "    tokens = prepare_text_for_lda(tweets.text[i])\n",
    "    if random.random() > .9: #random select around 1000 text\n",
    "        text_data.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic analysis\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(text_data) \n",
    "corpus =[dictionary.doc2bow(text) for text in text_data]\n",
    "\n",
    "import pickle\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')\n",
    "\n",
    "from itertools import chain\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 3, id2word=dictionary, passes=15)\n",
    "ldamodel.save('model3.gensim')\n",
    "lda_corpus = ldamodel[corpus]\n",
    "scores = list(chain(*[[score for topic_id,score in topic] \\\n",
    "                      for topic in [doc for doc in lda_corpus]]))\n",
    "\n",
    "threshold = sum(scores)/len(scores)\n",
    "tweet_list=list(tweets.text)\n",
    "\n",
    "cluster1 = [j for i,j in zip(lda_corpus,tweet_list) if i[0][1] > threshold]\n",
    "cluster2 = [j for i,j in zip(lda_corpus,tweet_list) if i[1][1] > threshold]\n",
    "cluster3 = [j for i,j in zip(lda_corpus,tweet_list) if i[2][1] > threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1149"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cluster3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualisation of topic distribution\n",
    "dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')\n",
    "corpus = pickle.load(open('corpus.pkl', 'rb'))\n",
    "lda3 = gensim.models.ldamodel.LdaModel.load('model3.gensim')\n",
    "import pyLDAvis.gensim\n",
    "lda_display3 = pyLDAvis.gensim.prepare(lda3, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display3)\n",
    "\n",
    "pyLDAvis.save_html(lda_display3, 'lda.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_time</th>\n",
       "      <th>suspend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Necessity is the mother of invention</td>\n",
       "      <td>2019-10-18 21:21</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#病毒  Constantly defying the will of the citize...</td>\n",
       "      <td>2020-03-26 05:09</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#肺炎  #病毒  The priceless life should not be the...</td>\n",
       "      <td>2020-03-28 02:37</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Want to see the walking dead, but feel a littl...</td>\n",
       "      <td>2019-10-30 05:57</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Maybe should call that Kekong? #HongKong #Keki...</td>\n",
       "      <td>2020-01-03 01:39</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32921</th>\n",
       "      <td>Look before you leap. First think, then act.</td>\n",
       "      <td>2019-05-22 08:30</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32922</th>\n",
       "      <td>I was for all intensive purposes addicted to t...</td>\n",
       "      <td>2019-04-21 03:23</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32923</th>\n",
       "      <td>.Live a noble and honest life.Reviving past ti...</td>\n",
       "      <td>2019-06-17 08:48</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32924</th>\n",
       "      <td>her, and she drooped and would have sunk down ...</td>\n",
       "      <td>2019-10-24 01:19</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32925</th>\n",
       "      <td>Whаtever you do, do it well. – Wаlt Disney</td>\n",
       "      <td>2019-10-11 10:16</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32926 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweet_text        tweet_time  \\\n",
       "0                   Necessity is the mother of invention  2019-10-18 21:21   \n",
       "1      #病毒  Constantly defying the will of the citize...  2020-03-26 05:09   \n",
       "2      #肺炎  #病毒  The priceless life should not be the...  2020-03-28 02:37   \n",
       "3      Want to see the walking dead, but feel a littl...  2019-10-30 05:57   \n",
       "4      Maybe should call that Kekong? #HongKong #Keki...  2020-01-03 01:39   \n",
       "...                                                  ...               ...   \n",
       "32921       Look before you leap. First think, then act.  2019-05-22 08:30   \n",
       "32922  I was for all intensive purposes addicted to t...  2019-04-21 03:23   \n",
       "32923  .Live a noble and honest life.Reviving past ti...  2019-06-17 08:48   \n",
       "32924  her, and she drooped and would have sunk down ...  2019-10-24 01:19   \n",
       "32925         Whаtever you do, do it well. – Wаlt Disney  2019-10-11 10:16   \n",
       "\n",
       "       suspend  \n",
       "0         True  \n",
       "1         True  \n",
       "2         True  \n",
       "3         True  \n",
       "4         True  \n",
       "...        ...  \n",
       "32921     True  \n",
       "32922     True  \n",
       "32923     True  \n",
       "32924     True  \n",
       "32925     True  \n",
       "\n",
       "[32926 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "country='China'\n",
    "tweets=tweets.drop(['text'], axis=1)\n",
    "tweets.to_csv('/Volumes/BECCA/Project/Data/Input/02_Preprocessing/01_Suspend %s.csv' %country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keyword list\n",
    "keyword1=[]\n",
    "keyword2=[]\n",
    "keyword3=[]\n",
    "keywords=[keyword1,keyword2,keyword3]\n",
    "\n",
    "for idx, topic in ldamodel.show_topics(formatted=False, num_words= 2): #2 topics would make api find result much more easiler\n",
    "    for w in topic:\n",
    "        if idx==0:\n",
    "            keyword1.append(w[0])\n",
    "        elif idx==1:\n",
    "            keyword2.append(w[0])\n",
    "        elif idx==2:\n",
    "            keyword3.append(w[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['milesguo', 'wengui'], ['milesguo', 'china'], ['world', 'epidemic']]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Russia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/becca/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3254: DtypeWarning: Columns (0,1,9,16,17,19,20) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/Users/becca/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3254: DtypeWarning: Columns (0,1,9,17,19,20) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    }
   ],
   "source": [
    "#Read Files\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def file_reader(path):\n",
    "    df=pd.DataFrame()\n",
    "    chunksize = 10 ** 6\n",
    "    for chunk in pd.read_csv(path, chunksize=chunksize):\n",
    "        df = df.append(chunk)\n",
    "    return df\n",
    "df=file_reader('/Volumes/BECCA/Project/Data/Input/01_Raw data/Russia_052020.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop columns, expect 'text' & 'tweet_language' & 'tweet_time'\n",
    "for i in df:\n",
    "    if i != 'tweet_text' :\n",
    "        if i != 'tweet_language':\n",
    "            if i != 'tweet_time':\n",
    "                df=df.drop([i], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just use the tweet in English\n",
    "tweets=[]\n",
    "for i in range(len(df)):\n",
    "    if df.tweet_language[i] == 'en':\n",
    "        l=[]\n",
    "        l.append(df.tweet_text[i])\n",
    "        l.append(df.tweet_time[i])\n",
    "        tweets.append(l)\n",
    "tweets=pd.DataFrame(tweets, columns=['tweet_text','tweet_time'])\n",
    "tweets['suspend']=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweet_text preprocessing\n",
    "import preprocessor as p\n",
    "\n",
    "for i,v in enumerate(tweets['tweet_text']):\n",
    "    tweets.loc[i,'text'] = p.clean(v)\n",
    "    \n",
    "import spacy\n",
    "# spacy.load('en_core_web_sm')\n",
    "from spacy.lang.en import English\n",
    "parser = English()\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens\n",
    "\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "import random\n",
    "text_data = []\n",
    "for i in range(len(tweets)):\n",
    "    tokens = prepare_text_for_lda(tweets.text[i])\n",
    "    if random.random() > .9: #random select around 1000 text\n",
    "        text_data.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic analysis\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(text_data) \n",
    "corpus =[dictionary.doc2bow(text) for text in text_data]\n",
    "\n",
    "import pickle\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')\n",
    "\n",
    "from itertools import chain\n",
    "# ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 3, id2word=dictionary, \\\n",
    "#                                            update_every=1, chunksize=10000, passes=15)\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 3, id2word=dictionary, passes=15)\n",
    "ldamodel.save('model3.gensim')\n",
    "lda_corpus = ldamodel[corpus]\n",
    "scores = list(chain(*[[score for topic_id,score in topic] \\\n",
    "                      for topic in [doc for doc in lda_corpus]]))\n",
    "\n",
    "threshold = sum(scores)/len(scores)\n",
    "tweet_list=list(tweets.text)\n",
    "\n",
    "\n",
    "cluster1 = [j for i,j in zip(lda_corpus,tweet_list) if i[0][1] > threshold]\n",
    "cluster2 = [j for i,j in zip(lda_corpus,tweet_list) if i[1][1] > threshold]\n",
    "cluster3 = [j for i,j in zip(lda_corpus,tweet_list) if i[2][1] > threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualisation of topic distribution\n",
    "dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')\n",
    "corpus = pickle.load(open('corpus.pkl', 'rb'))\n",
    "lda3 = gensim.models.ldamodel.LdaModel.load('model3.gensim')\n",
    "import pyLDAvis.gensim\n",
    "lda_display3 = pyLDAvis.gensim.prepare(lda3, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display3)\n",
    "\n",
    "country='Russia'\n",
    "pyLDAvis.save_html(lda_display3, 'lda_%s.html' %country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "country='Russia'\n",
    "tweets=tweets.drop(['text'], axis=1)\n",
    "tweets.to_csv('/Volumes/BECCA/Project/Data/Input/02_Preprocessing/01_Suspend %s.csv' %country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keyword list\n",
    "keyword1=[]\n",
    "keyword2=[]\n",
    "keyword3=[]\n",
    "keywords=[keyword1,keyword2,keyword3]\n",
    "\n",
    "for idx, topic in ldamodel.show_topics(formatted=False, num_words= 2): #2 topics would make api find result much more easiler\n",
    "    for w in topic:\n",
    "        if idx==0:\n",
    "            keyword1.append(w[0])\n",
    "        elif idx==1:\n",
    "            keyword2.append(w[0])\n",
    "        elif idx==2:\n",
    "            keyword3.append(w[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['people', 'football'], ['girl', 'naked'], ['followers', 'stats']]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
